{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec,TaggedDocument \n",
    "import string as string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data source---https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection --\n",
    "Dataset author--- Rishabh Misra\n",
    "Short description about Dataset: \n",
    "            News headlines Sarcasm dataset was developed from two news websites 'The Onion' and 'Huffpost'. First site provides the sarcastic or satirical way of current news and whereas the second provides non sarcastic news headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>26709</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0  26709\n",
       "label       \n",
       "0      15072\n",
       "1      11637"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--------Inputing the file---\n",
    "news = pd.read_json('news.json',lines= True)\n",
    "news = news.rename(columns={'headline':'comment','is_sarcastic':'label'})\n",
    "news = news[['label','comment']]\n",
    "news = news.sample(n = 26709, replace = \"False\",random_state=2)\n",
    "sample = news\n",
    "sample.dropna(inplace=True)                         #--------removing NULL values in dataset if any---\n",
    "sample.reset_index(drop=True, inplace=True)         #--------resetting the index after removing NULL values---\n",
    "\n",
    "#--------Checking Class Balance in the dataset using crosstab-------\n",
    "pd.crosstab(sample['label'],len(sample['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------Removing Numbers from the text and leaving all other features like capital words, punctuation marks etc as it plays role in sarcasm detection-------\n",
    "sample['no_numb'] = sample.comment.str.replace('[0-9]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----Function to create Paragraph vector--------\n",
    "#----Refered https://gist.github.com/susanli2016/801918d5ff37f52b55cce0a46088706c\n",
    "\n",
    "from gensim.models import doc2vec\n",
    "\n",
    "def tag_text(corpus, label_type):\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(doc2vec.TaggedDocument(v.split(), [label]))\n",
    "    return labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------Spliting the data to train and test also assigning tags to each row----------\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample.no_numb, sample.label, random_state=2, test_size=0.25)\n",
    "X_train = tag_text(X_train, 'Train')\n",
    "X_test = tag_text(X_test, 'Test')\n",
    "all_text = X_train + X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1979391.93it/s]\n"
     ]
    }
   ],
   "source": [
    "#-------Building Paragraph vector based on Distributed Bag of words(PV-DBOW) model---\n",
    "\n",
    "from tqdm import tqdm\n",
    "dbow_method = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065, window = 11)\n",
    "dbow_method.build_vocab([x for x in tqdm(all_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 2213595.98it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1536092.17it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1978517.96it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1604975.22it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1477911.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1264155.47it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1391277.52it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1505822.51it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 2131873.06it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 4155254.66it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1399026.72it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1104223.33it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1260726.84it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1191889.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1544010.28it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1281083.93it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1385119.14it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1607531.65it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1648357.40it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1596489.46it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1347628.54it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1076983.46it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1072026.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1350846.08it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1225543.06it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1571782.66it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1383630.77it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1085362.26it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1845927.79it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1642340.17it/s]\n"
     ]
    }
   ],
   "source": [
    "#------Training the PV-DBOW model-----------\n",
    "from sklearn import utils\n",
    "\n",
    "for epoch in range(30):\n",
    "    dbow_method.train(utils.shuffle([x for x in tqdm(all_text)]), total_examples=len(all_text), epochs=1)\n",
    "    dbow_method.alpha -= 0.002\n",
    "    dbow_method.min_alpha = dbow_method.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----Function to extract Vectors-----------\n",
    "#-----Refered https://github.com/susanli2016/NLP-with-Python\n",
    "\n",
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dbow = get_vectors(dbow_method, len(X_train), 300, 'Train')\n",
    "test_dbow = get_vectors(dbow_method, len(X_test), 300, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------Parameter Tuning using RandomizedSearchCV-------------\n",
    "#-----------refered from https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html--\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "clf_r= SVC()  \n",
    "param_dist_r = {\"C\": [1,2,3,4,5,6,7,8,9,10],\n",
    "              \"kernel\": [\"rbf\"],\n",
    "              \"gamma\":[0.1,0.2,0.3,0.4,0.5,0.6]\n",
    "              }\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 2\n",
    "random_search_r = RandomizedSearchCV(clf_r, param_distributions=param_dist_r,\n",
    "                                   n_iter=n_iter_search, scoring= 'accuracy', cv=5, iid=False)\n",
    "\n",
    "random_result_rbf = random_search_r.fit(train_dbow,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC  \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "clf_l = SVC()  \n",
    "param_dist_l = {\"C\": [1,2,3,4,5,6,7,8,9,10],\n",
    "              \"kernel\": [\"linear\"]\n",
    "              }\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 2\n",
    "random_search_l = RandomizedSearchCV(clf_l, param_distributions=param_dist_l,\n",
    "                                   n_iter=n_iter_search, scoring= 'accuracy', cv=5, iid=False)\n",
    "\n",
    "random_result_linear = random_search_l.fit(train_dbow,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kernel': 'rbf', 'gamma': 0.5, 'C': 5}\n",
      "0.8947369896437749\n",
      "{'kernel': 'linear', 'C': 3}\n",
      "0.8126874267906\n"
     ]
    }
   ],
   "source": [
    "print(random_result_rbf.best_params_)\n",
    "print(random_result_rbf.best_score_)\n",
    "print(random_result_linear.best_params_)\n",
    "print(random_result_linear.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score: 0.9126984126984127\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.92      3766\n",
      "           1       0.90      0.89      0.90      2912\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6678\n",
      "   macro avg       0.91      0.91      0.91      6678\n",
      "weighted avg       0.91      0.91      0.91      6678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#-------Modeling SVC-------\n",
    "\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "svclassifier = SVC(kernel='rbf',C=5,gamma=0.5)  \n",
    "rbf_vector = svclassifier.fit(train_dbow, y_train)\n",
    "y_pred = svclassifier.predict(test_dbow)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy_score:',accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score: 0.8153638814016172\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84      3766\n",
      "           1       0.80      0.77      0.78      2912\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      6678\n",
      "   macro avg       0.81      0.81      0.81      6678\n",
      "weighted avg       0.81      0.82      0.81      6678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#-------Modeling SVC-------\n",
    "\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "svclassifier = SVC(kernel='linear',C=3)  \n",
    "linear_vector = svclassifier.fit(train_dbow, y_train)\n",
    "y_pred = svclassifier.predict(test_dbow)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy_score:',accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rbf_vector_s.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "# Output a pickle file for the model\n",
    "joblib.dump(rbf_vector, 'rbf_vector_s.pkl') \n",
    "\n",
    "#joblib.dump(linear_vector, 'linear_vector_s.pkl') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1000, 'max_features': 'sqrt', 'max_depth': 50, 'criterion': 'entropy'}\n",
      "0.869384047037084\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "rfclassifier = RandomForestClassifier()  \n",
    "param_dist_rf = {'n_estimators':[100,400,600,1000,1200],'max_features':['sqrt'],'criterion':['gini','entropy'], \n",
    "               'max_depth' :[10,20,50,100,150] }\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 2\n",
    "random_search_rscv = RandomizedSearchCV(rfclassifier, param_distributions=param_dist_rf,\n",
    "                                   n_iter=n_iter_search, scoring= 'accuracy', cv=5, iid=False)\n",
    "\n",
    "random_result_rf = random_search_rscv.fit(train_dbow,y_train)\n",
    "\n",
    "print(random_result_rf.best_params_)\n",
    "print(random_result_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score: 0.8845462713387242\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90      3766\n",
      "           1       0.89      0.84      0.86      2912\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      6678\n",
      "   macro avg       0.89      0.88      0.88      6678\n",
      "weighted avg       0.88      0.88      0.88      6678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#--------Modelling Random Forest------\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create the model with 1000 trees\n",
    "model = RandomForestClassifier(n_estimators=1000, bootstrap = True, max_features = 'sqrt', max_depth=50, criterion='entropy')\n",
    "# Fit on training data\n",
    "rf_vector = model.fit(train_dbow,y_train)\n",
    "\n",
    "rf_predictions = model.predict(test_dbow)\n",
    "print('Accuracy_score:',accuracy_score(y_test,rf_predictions))\n",
    "print(classification_report(y_test,rf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rf_vector_s.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "# Output a pickle file for the model\n",
    "joblib.dump(rf_vector, 'rf_vector_s.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "lrclassifier1 = LogisticRegression()  \n",
    "param_dist_lr1 = {'max_iter':[20,60,100,150,200,300,400,500],'solver':['newton-cg', 'sag','lbfgs'],'penalty':['l2']}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 2\n",
    "random_search_lrcv1 = RandomizedSearchCV(lrclassifier1, param_distributions=param_dist_lr1,\n",
    "                                   n_iter=n_iter_search, scoring= 'accuracy', cv=5, iid=False, n_jobs= -1, random_state= 22)\n",
    "\n",
    "random_result_lr1 = random_search_lrcv1.fit(train_dbow,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\NCI Notes\\Sem 2 Notes\\New folder\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "lrclassifier2 = LogisticRegression()  \n",
    "param_dist_lr2 = {'max_iter':[20,60,100,150,200,300,400,500],'solver':['liblinear', 'saga'],'penalty':['l1']}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 2\n",
    "random_search_lrcv2 = RandomizedSearchCV(lrclassifier2, param_distributions=param_dist_lr2,\n",
    "                                   n_iter=n_iter_search, scoring= 'accuracy', cv=5, iid=False, n_jobs= -1, random_state= 22)\n",
    "\n",
    "random_result_lr2 = random_search_lrcv2.fit(train_dbow,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'lbfgs', 'penalty': 'l2', 'max_iter': 200}\n",
      "0.8107084453076618\n",
      "{'solver': 'saga', 'penalty': 'l1', 'max_iter': 60}\n",
      "0.8100665767534858\n"
     ]
    }
   ],
   "source": [
    "print(random_result_lr1.best_params_)\n",
    "print(random_result_lr1.best_score_)\n",
    "print(random_result_lr2.best_params_)\n",
    "print(random_result_lr2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score: 0.8138664270739743\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.85      0.84      3766\n",
      "           1       0.80      0.77      0.78      2912\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6678\n",
      "   macro avg       0.81      0.81      0.81      6678\n",
      "weighted avg       0.81      0.81      0.81      6678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#--------Modelling Logistic Regression------\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(solver='lbfgs',penalty='l2',max_iter=200, random_state=22)\n",
    "lr_vector = classifier.fit(train_dbow, y_train)\n",
    "\n",
    "lr_predictions = classifier.predict(test_dbow)\n",
    "print('Accuracy_score:',accuracy_score(y_test,lr_predictions))\n",
    "print(classification_report(y_test,lr_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lr_vector_s.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "# Output a pickle file for the model\n",
    "joblib.dump(lr_vector, 'lr_vector_s.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 3800701.12it/s]\n"
     ]
    }
   ],
   "source": [
    "#-------Building Paragraph vector based on Distributed memory(PV-DM) model---\n",
    "\n",
    "dm_method = Doc2Vec(dm=1, dm_mean=1, vector_size=300, window=8, negative=5,\n",
    "                   min_count=1, workers=5, alpha=0.065, min_alpha=0.065)\n",
    "dm_method.build_vocab([x for x in tqdm(all_text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 3334593.41it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1373873.75it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1795311.87it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 3347547.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 4462996.12it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 4486949.39it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 3821184.48it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 2200810.69it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1335562.72it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 2952083.52it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 2502863.46it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 3842548.73it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 3344748.62it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 3819751.28it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1409606.60it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1298833.24it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 2974659.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 3781329.42it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1269700.39it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 4057283.89it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 3122753.68it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1229699.95it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 4474761.95it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 3843207.85it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 2188214.97it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 2029119.63it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 3370612.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 3577037.66it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 1285302.33it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 26709/26709 [00:00<00:00, 3824445.77it/s]\n"
     ]
    }
   ],
   "source": [
    "#------Training the PV-DM model-----------\n",
    "for epoch in range(30):\n",
    "    dm_method.train(utils.shuffle([x for x in tqdm(all_text)]), total_examples=len(all_text), epochs=1)\n",
    "    dm_method.alpha -= 0.002\n",
    "    dm_method.min_alpha = dm_method.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dm = get_vectors(dm_method, len(X_train), 300, 'Train')\n",
    "test_dm = get_vectors(dm_method, len(X_test), 300, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC  \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "clf_r= SVC()  \n",
    "param_dist_r = {\"C\": [1,2,3,4,5,6,7,8,9,10],\n",
    "              \"kernel\": [\"rbf\"],\n",
    "              \"gamma\":[0.1,0.2,0.3,0.4,0.5,0.6]\n",
    "              }\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 2\n",
    "random_search_r = RandomizedSearchCV(clf_r, param_distributions=param_dist_r,\n",
    "                                   n_iter=n_iter_search, scoring= 'accuracy', cv=5, iid=False)\n",
    "\n",
    "random_result_rbf_dm = random_search_r.fit(train_dm,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC  \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "clf_l = SVC()  \n",
    "param_dist_l = {\"C\": [1,2,3,4,5,6,7,8,9,10],\n",
    "              \"kernel\": [\"linear\"]\n",
    "              }\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 2\n",
    "random_search_l = RandomizedSearchCV(clf_l, param_distributions=param_dist_l,\n",
    "                                   n_iter=n_iter_search, scoring= 'accuracy', cv=5, iid=False)\n",
    "\n",
    "random_result_linear_dm = random_search_l.fit(train_dm,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kernel': 'rbf', 'gamma': 0.2, 'C': 6}\n",
      "0.820335944438803\n",
      "{'kernel': 'linear', 'C': 10}\n",
      "0.7733743329810224\n"
     ]
    }
   ],
   "source": [
    "print(random_result_rbf_dm.best_params_)\n",
    "print(random_result_rbf_dm.best_score_)\n",
    "print(random_result_linear_dm.best_params_)\n",
    "print(random_result_linear_dm.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score: 0.8511530398322851\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.88      0.87      3766\n",
      "           1       0.84      0.82      0.83      2912\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      6678\n",
      "   macro avg       0.85      0.85      0.85      6678\n",
      "weighted avg       0.85      0.85      0.85      6678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#-------Modeling SVC-------\n",
    "\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "svclassifier = SVC(kernel='rbf',C=6,gamma=0.2)  \n",
    "rbf_vector_dm_rbf = svclassifier.fit(train_dm, y_train)\n",
    "y_pred = svclassifier.predict(test_dm)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy_score:',accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score: 0.7791254866726565\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.83      0.81      3766\n",
      "           1       0.76      0.71      0.74      2912\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      6678\n",
      "   macro avg       0.78      0.77      0.77      6678\n",
      "weighted avg       0.78      0.78      0.78      6678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#-------Modeling SVC-------\n",
    "\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "svclassifier = SVC(kernel='linear',C=10)  \n",
    "rbf_vector_dm_linear = svclassifier.fit(train_dm, y_train)\n",
    "y_pred = svclassifier.predict(test_dm)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy_score:',accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rbf_vector_s_dm.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "# Output a pickle file for the model\n",
    "joblib.dump(rbf_vector_dm_rbf, 'rbf_vector_s_dm.pkl') \n",
    "\n",
    "#joblib.dump(rbf_vector_dm_linear, 'linear_vector_s_dm.pkl') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1000, 'max_features': 'sqrt', 'max_depth': 100, 'criterion': 'gini'}\n",
      "0.7886181211768425\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC  \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "rfclassifier = RandomForestClassifier()  \n",
    "param_dist_rf = {'n_estimators':[100,400,600,1000,1200],'max_features':['sqrt'],'criterion':['gini','entropy'], \n",
    "               'max_depth' :[10,20,50,100,150] }\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 2\n",
    "random_search_rscv = RandomizedSearchCV(rfclassifier, param_distributions=param_dist_rf,\n",
    "                                   n_iter=n_iter_search, scoring= 'accuracy', cv=5, iid=False)\n",
    "\n",
    "random_result_rf_dm = random_search_rscv.fit(train_dm,y_train)\n",
    "\n",
    "print(random_result_rf_dm.best_params_)\n",
    "print(random_result_rf_dm.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score: 0.7999401018268942\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.88      0.83      3766\n",
      "           1       0.82      0.70      0.75      2912\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6678\n",
      "   macro avg       0.80      0.79      0.79      6678\n",
      "weighted avg       0.80      0.80      0.80      6678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#--------Modelling Random Forest------\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create the model with 1000 trees\n",
    "model = RandomForestClassifier(n_estimators=1000, bootstrap = True, max_features = 'sqrt', max_depth=100, criterion='gini')\n",
    "# Fit on training data\n",
    "rf_vector_dm = model.fit(train_dm,y_train)\n",
    "\n",
    "rf_predictions_dm = model.predict(test_dm)\n",
    "print('Accuracy_score:',accuracy_score(y_test,rf_predictions_dm))\n",
    "print(classification_report(y_test,rf_predictions_dm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rf_vector_s_dm.pkl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "# Output a pickle file for the model\n",
    "joblib.dump(rf_vector_dm, 'rf_vector_s_dm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "lrclassifier1 = LogisticRegression()  \n",
    "param_dist_lr1 = {'max_iter':[20,60,100,150,200,300,400,500],'solver':['newton-cg', 'sag','lbfgs'],'penalty':['l2']}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 2\n",
    "random_search_lrcv1 = RandomizedSearchCV(lrclassifier1, param_distributions=param_dist_lr1,\n",
    "                                   n_iter=n_iter_search, scoring= 'accuracy', cv=5, iid=False, n_jobs= -1, random_state= 22)\n",
    "\n",
    "random_result_lr1_dm = random_search_lrcv1.fit(train_dm,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\NCI Notes\\Sem 2 Notes\\New folder\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "lrclassifier2 = LogisticRegression()  \n",
    "param_dist_lr2 = {'max_iter':[20,60,100,150,200,300,400,500],'solver':['liblinear', 'saga'],'penalty':['l1']}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 2\n",
    "random_search_lrcv2 = RandomizedSearchCV(lrclassifier2, param_distributions=param_dist_lr2,\n",
    "                                   n_iter=n_iter_search, scoring= 'accuracy', cv=5, iid=False, n_jobs= -1, random_state= 22)\n",
    "\n",
    "random_result_lr2_dm = random_search_lrcv2.fit(train_dm,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'lbfgs', 'penalty': 'l2', 'max_iter': 200}\n",
      "0.7687211256405599\n",
      "{'solver': 'saga', 'penalty': 'l1', 'max_iter': 60}\n",
      "0.7699513582086777\n"
     ]
    }
   ],
   "source": [
    "print(random_result_lr1_dm.best_params_)\n",
    "print(random_result_lr1_dm.best_score_)\n",
    "print(random_result_lr2_dm.best_params_)\n",
    "print(random_result_lr2_dm.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\NCI Notes\\Sem 2 Notes\\New folder\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score: 0.7707397424378556\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.83      0.80      3766\n",
      "           1       0.76      0.70      0.73      2912\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      6678\n",
      "   macro avg       0.77      0.76      0.76      6678\n",
      "weighted avg       0.77      0.77      0.77      6678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#--------Modelling Logistic Regression------\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(solver='saga',penalty='l1',max_iter=60, random_state=22)\n",
    "lr_vector_dm = classifier.fit(train_dm, y_train)\n",
    "\n",
    "lr_predictions_dm = classifier.predict(test_dm)\n",
    "print('Accuracy_score:',accuracy_score(y_test,lr_predictions_dm))\n",
    "print(classification_report(y_test,lr_predictions_dm))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
