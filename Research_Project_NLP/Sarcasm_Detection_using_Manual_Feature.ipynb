{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import vocab_helpers as helper\n",
    "import punctuation as punc\n",
    "import re, emoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data source---https://www.kaggle.com/rmisra/news-headlines-dataset-for-sarcasm-detection --\n",
    "Dataset author--- Rishabh Misra\n",
    "Short description about Dataset: \n",
    "            News headlines Sarcasm dataset was developed from two news websites 'The Onion' and 'Huffpost'. First site provides the sarcastic or satirical way of current news and whereas the second provides non sarcastic news headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------Inputing the file---\n",
    "news = pd.read_json('news.json',lines= True)\n",
    "news = news.rename(columns={'headline':'comment','is_sarcastic':'label'})\n",
    "news = news[['label','comment']]\n",
    "news = news.sample(n = 26709, replace = \"False\",random_state=2)\n",
    "sample = news\n",
    "sample.dropna(inplace=True)                         #--------removing NULL values in dataset if any---\n",
    "sample.reset_index(drop=True, inplace=True)         #--------resetting the index after removing NULL values---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0  count\n",
       "label       \n",
       "0      15072\n",
       "1      11637"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--------Checking Class Balance in the dataset using crosstab-------\n",
    "pd.crosstab(sample['label'], columns= \"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------Removing Numbers from the text and leaving all other features like capital words, punctuation marks etc as it plays role in sarcasm detection-------\n",
    "sample['no_numb'] = sample.comment.str.replace('[0-9]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Darshan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Darshan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#--------Tokenizing the words for POS tag extration-----------------\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import word_tokenize, pos_tag\n",
    "sample['tokenized'] = sample['comment'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------Counting different POS tags present in the sentences---------\n",
    "\n",
    "#----Adjective Count---\n",
    "sample['adjective'] = sample['tokenized'].apply(lambda x: len([i for i in pos_tag(x) if i[1] in (\"JJ\",\"JJR\", \"JJS\")]))\n",
    "\n",
    "#----Adverbs Count------\n",
    "sample['adverb'] = sample['tokenized'].apply(lambda x: len([i for i in pos_tag(x) if i[1] in (\"RB\", \"RBR\", \"RBS\")]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------Extrating Positive and Negative score from VaderSentiment module----\n",
    "#------Refered from http://t-redactyl.io/blog/2017/04/applying-sentiment-analysis-with-vader-and-the-twitter-api.html\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "pos = []\n",
    "neg = []\n",
    "\n",
    "for i in range(0, len(sample)):\n",
    "    pos.append(analyzer.polarity_scores(sample.comment[i])['pos'])\n",
    "    neg.append(analyzer.polarity_scores(sample.comment[i])['neg'])\n",
    "\n",
    "sample['positive_score'] = pos\n",
    "sample['negative_score'] = neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------Function to Count Intensifier and Capital words-----\n",
    "#------Refered from https://github.com/MirunaPislar/Sarcasm-Detection\n",
    "\n",
    "def get_pragmatic_features(text_tokens):\n",
    "    capitalized_words = intensifiers =tweet_len_ch= 0\n",
    "    for t in text_tokens:\n",
    "        tweet_len_ch += len(t)\n",
    "        if t.isupper() and len(t) > 1:\n",
    "            capitalized_words += 1       # count of capitalized words\n",
    "        if t in helper.strong_negations:\n",
    "            intensifiers += 1           # count-based feature of strong negations\n",
    "        if t in helper.strong_affirmatives:\n",
    "            intensifiers += 1           # count-based feature of strong affirmatives\n",
    "        if t in helper.interjections:\n",
    "            intensifiers += 1           # count-based feature of relevant interjections\n",
    "        if t in helper.intensifiers:\n",
    "            intensifiers += 1           # count-based feature of relevant intensifiers\n",
    "    feature_list = {'capitalized': capitalized_words,'intensifiers': intensifiers}\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------Function to count number of punctuation marks used----\n",
    "def get_punc_features(text_tokens):\n",
    "    pat = r'[.?\\\",!*]+'\n",
    "    punc_param = 0\n",
    "    for t in text_tokens:\n",
    "        pattern = re.findall(pat,t)\n",
    "        for i in pattern:\n",
    "            punc_param += 1           # count-based feature of strong negations\n",
    "    punc_list = {'punc_param':punc_param}\n",
    "    return punc_list\n",
    "\n",
    "#-------Function to count Emoji-----------\n",
    "def get_emoji_features(text_tokens):\n",
    "    emo_pat = r'[:)\\(]+'\n",
    "    emoji_param = 0\n",
    "    for t in text_tokens:\n",
    "        pattern_e = re.findall(emo_pat,t)\n",
    "        for i in pattern_e:\n",
    "            emoji_param += 1           # count-based feature of strong negations\n",
    "    emoji_list = {'emoji_param':emoji_param}\n",
    "    return emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------Function to count Comparison words-----------\n",
    "\n",
    "import comparison as comp\n",
    "def get_like_features(text_tokens):\n",
    "    like_param = 0\n",
    "    for t in text_tokens:\n",
    "        if t in comp.comparison:\n",
    "            like_param += 1           # count-based feature of strong negations\n",
    "    like_list = {'like_param':like_param}\n",
    "    return like_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------Calling functions for Capital, Intensifier, Comparison, Punctuation and Emoji-------\n",
    "\n",
    "cap = []\n",
    "intensifier = []\n",
    "vs_comp = []\n",
    "punc = []\n",
    "emoji = []\n",
    "\n",
    "for i in range(0, len(sample)):\n",
    "        cap.append(get_pragmatic_features(sample.tokenized[i])['capitalized'])\n",
    "        intensifier.append(get_pragmatic_features(sample.tokenized[i])['intensifiers'])\n",
    "        vs_comp.append(get_like_features(sample.tokenized[i])['like_param'])\n",
    "        punc.append(get_punc_features(sample.tokenized[i])['punc_param'])\n",
    "        emoji.append(get_emoji_features(sample.tokenized[i])['emoji_param'])\n",
    "    \n",
    "sample['capital'] = cap\n",
    "sample['intensifier'] = intensifier\n",
    "sample['like_param'] = vs_comp\n",
    "sample['punctuation'] = punc\n",
    "sample['emoji'] = emoji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- Checking the presence of unigrams-------\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer1 = CountVectorizer(ngram_range=(1,1), max_features = 100, max_df=1.0, min_df=0.0)\n",
    "count_vectors = vectorizer1.fit_transform(list(sample['no_numb']))\n",
    "\n",
    "# reshape to pandas\n",
    "from scipy import sparse\n",
    "vectors = pd.DataFrame(count_vectors.todense())\n",
    "vectors.columns = vectorizer1.get_feature_names()\n",
    "unigram_features = vectorizer1.get_feature_names()\n",
    "sample = pd.concat([sample.reset_index(drop=True),vectors.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- Checking the presence of bigrams-------\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer2 = CountVectorizer(ngram_range=(2,2), max_features = 50, max_df=1.0, min_df=0.0)\n",
    "count_vectors2 = vectorizer2.fit_transform(list(sample['no_numb']))\n",
    "\n",
    "# reshape to pandas\n",
    "from scipy import sparse\n",
    "vectors2 = pd.DataFrame(count_vectors2.todense())\n",
    "vectors2.columns = vectorizer2.get_feature_names()\n",
    "bigram_features = vectorizer2.get_feature_names()\n",
    "sample = pd.concat([sample.reset_index(drop=True),vectors2.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_input = sample[['label', 'adjective', 'adverb',\n",
    "       'positive_score', 'negative_score', 'capital', 'intensifier',\n",
    "       'like_param', 'punctuation', 'emoji']+unigram_features\n",
    "                     +bigram_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "#-------Separating Labels and Independent columns--------\n",
    "X = model_input.drop('label', axis=1)  \n",
    "y = model_input['label']  \n",
    "\n",
    "#-------Dividing input into training and testing set------\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------Parameter Tuning using RandomizedSearchCV-------------\n",
    "#-----------refered from https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html--\n",
    "\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "clf_r= SVC()  \n",
    "param_dist_r = {\"C\": [1,2,3,4,5,6,7,8,9,10],\n",
    "              \"kernel\": [\"rbf\"],\n",
    "              \"gamma\":[0.1,0.2,0.3,0.4,0.5,0.6]\n",
    "              }\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 2\n",
    "random_search_r = RandomizedSearchCV(clf_r, param_distributions=param_dist_r,\n",
    "                                   n_iter=n_iter_search, scoring= 'accuracy', cv=5, iid=False)\n",
    "\n",
    "random_result_rbf = random_search_r.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC  \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "clf_l = SVC()  \n",
    "param_dist_l = {\"C\": [1,2,3,4,5,6,7,8,9,10],\n",
    "              \"kernel\": [\"linear\"]\n",
    "              }\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 2\n",
    "random_search_l = RandomizedSearchCV(clf_l, param_distributions=param_dist_l,\n",
    "                                   n_iter=n_iter_search, scoring= 'accuracy', cv=5, iid=False)\n",
    "\n",
    "random_result_linear = random_search_l.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kernel': 'rbf', 'gamma': 0.2, 'C': 8}\n",
      "0.8372522449168773\n",
      "{'kernel': 'linear', 'C': 3}\n",
      "0.7701060712400329\n"
     ]
    }
   ],
   "source": [
    "print(random_result_rbf.best_params_)\n",
    "print(random_result_rbf.best_score_)\n",
    "print(random_result_linear.best_params_)\n",
    "print(random_result_linear.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score: 0.8361784965558551\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.82      0.85      3737\n",
      "           1       0.79      0.85      0.82      2941\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      6678\n",
      "   macro avg       0.83      0.84      0.84      6678\n",
      "weighted avg       0.84      0.84      0.84      6678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#-------Modeling SVC with rbf kernel-------\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "svclassifier = SVC(kernel='rbf',C=8,gamma=0.2, random_state= 22)  \n",
    "rbf_model_s = svclassifier.fit(X_train, y_train)\n",
    "y_pred = svclassifier.predict(X_test)\n",
    "\n",
    "#-------Predicting Accuracy-----\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy_score:',accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score: 0.7680443246480982\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.75      0.78      3737\n",
      "           1       0.71      0.79      0.75      2941\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      6678\n",
      "   macro avg       0.77      0.77      0.77      6678\n",
      "weighted avg       0.77      0.77      0.77      6678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#-------Modeling SVC with linear kernel-------\n",
    "from sklearn.svm import SVC  \n",
    "\n",
    "svclassifier = SVC(kernel='linear',C=3, random_state= 22)  \n",
    "linear_model_s = svclassifier.fit(X_train, y_train)\n",
    "y_pred = svclassifier.predict(X_test)\n",
    "\n",
    "#-------Predicting Accuracy-----\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('Accuracy_score:',accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['linear_model_s.pkl']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "# Output a pickle file for the model\n",
    "joblib.dump(rbf_model_s, 'rbf_model_s.pkl') \n",
    "\n",
    "joblib.dump(linear_model_s, 'linear_model_s.pkl') \n",
    "\n",
    "\n",
    "# Load the pickle file\n",
    "#clf_load = joblib.load('svc_model_mf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC  \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "rfclassifier = RandomForestClassifier()  \n",
    "param_dist_rf = {'n_estimators':[100,400,600,1000,1200],'max_features':['sqrt'],'criterion':['gini','entropy'], \n",
    "               'max_depth' :[10,20,50,100,150] }\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 2\n",
    "random_search_rscv = RandomizedSearchCV(rfclassifier, param_distributions=param_dist_rf,\n",
    "                                   n_iter=n_iter_search, scoring= 'accuracy', cv=5, iid=False)\n",
    "\n",
    "random_result_rf = random_search_rscv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 100, 'max_features': 'sqrt', 'max_depth': 100, 'criterion': 'entropy'}\n",
      "0.877989055847225\n"
     ]
    }
   ],
   "source": [
    "print(random_result_rf.best_params_)\n",
    "print(random_result_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score: 0.8679245283018868\n"
     ]
    }
   ],
   "source": [
    "#--------Modelling Random Forest------\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create the model with 100 trees\n",
    "model = RandomForestClassifier(n_estimators=100, bootstrap = True,max_features = 'sqrt',criterion='entropy',max_depth=100, random_state = 22)\n",
    "# Fit on training data\n",
    "rf_model = model.fit(X_train,y_train)\n",
    "\n",
    "rf_predictions = model.predict(X_test)\n",
    "print('Accuracy_score:',accuracy_score(y_test,rf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.88      0.88      3737\n",
      "           1       0.85      0.85      0.85      2941\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      6678\n",
      "   macro avg       0.87      0.87      0.87      6678\n",
      "weighted avg       0.87      0.87      0.87      6678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,rf_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rf_model_s.pkl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "# Output a pickle file for the model\n",
    "joblib.dump(rf_model, 'rf_model_s.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "lrclassifier1 = LogisticRegression()  \n",
    "param_dist_lr1 = {'max_iter':[20,60,100,150,200,300,400,500],'solver':['newton-cg', 'sag','lbfgs'],'penalty':['l2']}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 2\n",
    "random_search_lrcv1 = RandomizedSearchCV(lrclassifier1, param_distributions=param_dist_lr1,\n",
    "                                   n_iter=n_iter_search, scoring= 'accuracy', cv=5, iid=False, n_jobs= -1, random_state= 22)\n",
    "\n",
    "random_result_lr1 = random_search_lrcv1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\NCI Notes\\Sem 2 Notes\\New folder\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "lrclassifier2 = LogisticRegression()  \n",
    "param_dist_lr2 = {'max_iter':[20,60,100,150,200,300,400,500],'solver':['liblinear', 'saga'],'penalty':['l1']}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 2\n",
    "random_search_lrcv2 = RandomizedSearchCV(lrclassifier2, param_distributions=param_dist_lr2,\n",
    "                                   n_iter=n_iter_search, scoring= 'accuracy', cv=5, iid=False, n_jobs= -1, random_state= 22)\n",
    "\n",
    "random_result_lr2 = random_search_lrcv2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'solver': 'newton-cg', 'penalty': 'l2', 'max_iter': 150}\n",
      "0.7707050978311668\n",
      "{'solver': 'saga', 'penalty': 'l1', 'max_iter': 60}\n",
      "0.7366578906284945\n"
     ]
    }
   ],
   "source": [
    "print(random_result_lr1.best_params_)\n",
    "print(random_result_lr1.best_score_)\n",
    "print(random_result_lr2.best_params_)\n",
    "print(random_result_lr2.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_score: 0.77088948787062\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.81      0.80      3737\n",
      "           1       0.75      0.72      0.74      2941\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      6678\n",
      "   macro avg       0.77      0.77      0.77      6678\n",
      "weighted avg       0.77      0.77      0.77      6678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#--------Modelling Logistic Regression------\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(solver='newton-cg', penalty='l2',max_iter=150, random_state= 223)\n",
    "\n",
    "lr_model = classifier.fit(X_train, y_train)\n",
    "\n",
    "lr_predictions = classifier.predict(X_test)\n",
    "print('Accuracy_score:',accuracy_score(y_test,lr_predictions))\n",
    "print(classification_report(y_test,lr_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
